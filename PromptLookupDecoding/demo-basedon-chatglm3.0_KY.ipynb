{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "model_path = \"../chatglm3-6b-base\"\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).float()\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).half().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重写ChatGLM3-6b-Base的stream_generate()函数来嵌入Prompt Lookup Decoding函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import warnings\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "sys.path.append('../chatglm3-6b-base')\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, LayerNorm\n",
    "from torch.nn import CrossEntropyLoss, LayerNorm, MSELoss, BCEWithLogitsLoss\n",
    "from torch.nn.utils import skip_init\n",
    "from typing import Optional, Tuple, Union, List, Callable, Dict, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import logging\n",
    "from transformers.generation.logits_process import LogitsProcessor\n",
    "from transformers.generation.utils import LogitsProcessorList, StoppingCriteriaList, GenerationConfig, ModelOutput, _crop_past_key_values\n",
    "\n",
    "from configuration_chatglm import ChatGLMConfig\n",
    "\n",
    "### modify for Prompt Lookup Decoding\n",
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:end_idx]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def stream_generate_assisted_by_prompt_lookup_decoding(\n",
    "        self,\n",
    "        input_ids,\n",
    "        generation_config: Optional[GenerationConfig] = None,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "        return_past_key_values=False,\n",
    "        draft_matching_window_size = 3,\n",
    "        draft_num_candidate_tokens = 10,\n",
    "        **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    重新ChatGLM3-6b-base的stream_generate()函数\n",
    "    \"\"\"\n",
    "    batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n",
    "\n",
    "    if generation_config is None:\n",
    "        generation_config = self.generation_config\n",
    "    generation_config = copy.deepcopy(generation_config)\n",
    "    model_kwargs = generation_config.update(**kwargs)\n",
    "    model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
    "    bos_token_id, eos_token_id = generation_config.bos_token_id, generation_config.eos_token_id\n",
    "\n",
    "    if isinstance(eos_token_id, int):\n",
    "        eos_token_id = [eos_token_id]\n",
    "    eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "    has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
    "    if has_default_max_length and generation_config.max_new_tokens is None:\n",
    "        warnings.warn(\n",
    "            f\"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. \"\n",
    "            \"This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we\"\n",
    "            \" recommend using `max_new_tokens` to control the maximum length of the generation.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "    elif generation_config.max_new_tokens is not None:\n",
    "        generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length\n",
    "        if not has_default_max_length:\n",
    "            # logger.warn(\n",
    "            #     f\"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(=\"\n",
    "            #     f\"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. \"\n",
    "            #     \"Please refer to the documentation for more information. \"\n",
    "            #     \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\",\n",
    "            #     UserWarning,\n",
    "            # )\n",
    "            print(\n",
    "                f\"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(=\"\n",
    "                f\"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. \"\n",
    "                \"Please refer to the documentation for more information. \"\n",
    "                \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            \n",
    "\n",
    "    if input_ids_seq_length >= generation_config.max_length:\n",
    "        input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
    "        # logger.warning(\n",
    "        #     f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n",
    "        #     f\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\n",
    "        #     \" increasing `max_new_tokens`.\"\n",
    "        # )\n",
    "        print(\n",
    "            f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n",
    "            f\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\n",
    "            \" increasing `max_new_tokens`.\"\n",
    "        )\n",
    "\n",
    "    # 2. Set generation parameters if not already defined\n",
    "    logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "    stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "\n",
    "    logits_processor = self._get_logits_processor(\n",
    "        generation_config=generation_config,\n",
    "        input_ids_seq_length=input_ids_seq_length,\n",
    "        encoder_input_ids=input_ids,\n",
    "        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "        logits_processor=logits_processor,\n",
    "    )\n",
    "\n",
    "    stopping_criteria = self._get_stopping_criteria(\n",
    "        generation_config=generation_config, stopping_criteria=stopping_criteria\n",
    "    )\n",
    "    max_len = stopping_criteria[0].max_length\n",
    "\n",
    "    logits_warper = self._get_logits_warper(generation_config)\n",
    "\n",
    "    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "    scores = None\n",
    "    while True:\n",
    "        \n",
    "        cur_len = input_ids.shape[-1]\n",
    "        # print(f'cur_len: {cur_len}')\n",
    "\n",
    "        # 1. 从已有的input_ids中查找candidate_pred_tokens\n",
    "        # start_time = time.time()\n",
    "        candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "        # print(\"Lookup耗时=\", (time.time() - start_time))\n",
    "\n",
    "        if len(candidate_pred_tokens) == 0:\n",
    "            candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "        else:\n",
    "            candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "\n",
    "        # 2. 将候选token与input_ids做拼接\n",
    "        candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "        \n",
    "        candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "        # print('candidate_length: ', candidate_length)\n",
    "       \n",
    "\n",
    "        # 3. 构造模型输入\n",
    "        model_inputs = {\"return_last_logit\": False}\n",
    "        if model_kwargs[\"is_first_forward\"]:\n",
    "            # 初次运行，输入所有input_ids\n",
    "            model_inputs[\"input_ids\"] = candidate_input_ids\n",
    "            model_inputs[\"position_ids\"] = torch.unsqueeze(torch.range(0, candidate_input_ids.shape[1]-1, 1, dtype=torch.int64), dim=0)\n",
    "            model_inputs[\"attention_mask\"] = model_kwargs[\"attention_mask\"] \n",
    "            model_inputs[\"past_key_values\"] = None\n",
    "        else:\n",
    "            # 非初次运行，仅输入上一次推理的最后一个token和所有候选tokens\n",
    "            model_inputs[\"input_ids\"] = candidate_input_ids[:, min(-candidate_length-1, -1):]\n",
    "            model_inputs[\"position_ids\"] = torch.unsqueeze(torch.range(0, candidate_input_ids.shape[1]-1, 1, dtype=torch.int64), dim=0)[..., min(-candidate_length-1, -1):]\n",
    "            model_inputs[\"attention_mask\"] = None\n",
    "            model_inputs[\"past_key_values\"] = model_kwargs[\"past_key_values\"]\n",
    "\n",
    "        # print(f'inputs_ids: {model_inputs[\"input_ids\"].size()}')\n",
    "        # print(f'position_ids: {model_inputs[\"position_ids\"].size()}')\n",
    "        # if model_inputs[\"past_key_values\"] is not None:\n",
    "        #     print(f'past_key_values: {len(model_inputs[\"past_key_values\"])}')\n",
    "        #     print(f'past_key_values: {len(model_inputs[\"past_key_values\"][0])}')\n",
    "        #     print(f'past_key_values: {model_inputs[\"past_key_values\"][0][0].size()}')\n",
    "        # print('model_inputs: ', model_inputs)\n",
    "\n",
    "        # start_time = time.time()\n",
    "        # forward pass to get next token\n",
    "        outputs = self(\n",
    "            **model_inputs,\n",
    "            return_dict=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "        # print(\"模型推理耗时=\", (time.time() - start_time))\n",
    "\n",
    "        # print('outputs.logits', outputs.logits.size())\n",
    "\n",
    "        if model_kwargs[\"is_first_forward\"] :\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "        else:\n",
    "            new_logits = outputs.logits\n",
    "        # print(f'new_logits: {new_logits.size()}')\n",
    "        selected_tokens = new_logits.argmax(dim=-1)\n",
    "        # print(f'selected_tokens: {selected_tokens.size()}')\n",
    "        # print('*******-', tokenizer.decode(selected_tokens[0].tolist()))\n",
    "        candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "        # print(f'candidate_new_tokens: {candidate_new_tokens.size()}')\n",
    "        # print(tokenizer.decode(candidate_new_tokens[0].tolist()))\n",
    "        \n",
    "        # 4.查找llm输出与候选token匹配的项\n",
    "        #   查找策略：从第一个候选token开始逐一检查，一旦遇到不相互匹配的token，经保留最后一个不匹配的项，其它都舍弃\n",
    "        n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "        n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "        valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "        input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "        last_token = valid_tokens[: , -1]\n",
    "\n",
    "        new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "        new_cache_size = new_cur_len - 1\n",
    "        \n",
    "        # 5.根据第四步中的匹配结果（多少个候选token与LLM输出一致），抽取部分kv_cache\n",
    "        kv_caches = []\n",
    "        for idx in range(len(outputs.past_key_values)):\n",
    "            kv_caches.append((outputs.past_key_values[idx][0][:new_cache_size, :,:,:],\n",
    "                              outputs.past_key_values[idx][1][:new_cache_size, :,:,:]))\n",
    "        \n",
    "        model_kwargs[\"past_key_values\"] = tuple(kv_caches)\n",
    "        model_kwargs[\"is_first_forward\"] = False\n",
    "              \n",
    "        \n",
    "        unfinished_sequences = unfinished_sequences.mul(\n",
    "            last_token.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
    "        )\n",
    "        if return_past_key_values:\n",
    "            yield input_ids, outputs.past_key_values\n",
    "        else:\n",
    "            yield input_ids\n",
    "        # stop when each sentence is finished, or if we exceed the maximum length\n",
    "        if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n",
    "            break\n",
    "\n",
    "model.stream_generate_assisted_by_prompt_lookup_decoding = stream_generate_assisted_by_prompt_lookup_decoding.__get__(model, type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"\"\"<system>你是一个AI助手，旨在提供帮助、诚实和无害的对话服务。<user>风吹人，你能解释一下吗<assistant>风吹人是通过空调摄像头，计算出人体实时位置，仅对该区域进行送风。送风区域可以跟随人体走动而随时变化，人动，风也动。</s><user>风吹人功能在空调上要怎么进行操作<assistant>空调的风吹人功能是通过空调摄像头计算出人体的实时位置，然后仅对该区域进行送风。当人体移动时，送风区域也会随之改变，实现人动风也动的效果。</s><user>风避人的原理是什么，怎么做到的<assistant>风避人通过空调摄像头计算出人体实时位置，让风避开人体位置。避风区域能随人体走动而变化，人在哪，避风区域就在哪。</s><user>可供参考的知识库有：\n",
    "问题:智慧知冷暖是什么的答案是智慧知冷暖是在制冷和制热模式下，红外传感器在有效检测范围内检测房间空气温度和人体表温度，空调结合房间空气湿度、空调送风状态等参数，计算人体冷热感。空调根据冷热感，自动调整运行参数，不用人手动遥控空调，让房间空气温度达到让人体感到舒适的状态。\n",
    "问题:智慧知冷暖的运行参数是如何自动调整的的答案是智慧知冷暖通过红外传感器检测房间空气温度和人体表温度，结合房间空气湿度、空调送风状态等参数，计算人体冷热感。然后根据这些数据，自动调整运行参数，使得房间空气温度达到让人体感到舒适的状态，无需人工手动调整。\n",
    "知识点:风避人是通过空调摄像头计算出人体实时位置，让风避开人体位置，避风区域可以根据人体的走动而变化。\n",
    "知识点:空调的风避人功能是通过摄像头计算出人体实时位置，让风避开人体。避风区域可以随着人体走动而变化，实现人在哪，避风区域就在哪的效果。\n",
    "知识点:风避人是通过空调摄像头，计算出人体实时位置，让风避开人体位置。避风区域可以跟随人体走动而随时变化，人在哪里，避风区域就在哪里。\n",
    "知识点:风避人功能通过空调摄像头计算出人体实时位置，避风区域会跟随人体走动而变化，即人在哪里，避风区域就在哪里。\n",
    "知识点:风避人通过空调摄像头计算出人体实时位置，然后让风避开人体位置。避风区域能随人体走动而变化，人在哪，避风区域就在哪。\n",
    "问题:智慧知冷暖的运行方式是怎样的的答案是智慧知冷暖通过红外传感器检测房间空气温度和人体表温度，结合房间空气湿度和空调送风状态等参数，计算人体冷热感。然后根据这个冷热感，自动调整空调运行参数，使得房间温度达到人体感到舒适的状态，无需人手动遥控。\n",
    "问题:智慧知冷暖是怎么运作的的答案是智慧知冷暖通过红外传感器检测房间空气温度和人体表温度，结合房间空气湿度和空调送风状态等参数，计算人体冷热感。然后根据这个冷热感，自动调整空调运行参数，使得房间温度达到人体感到舒适的状态，无需人手动遥控。\n",
    "根据这些搜索结果回答关于空调的问题。如果知识库中没有答案，请拒绝回答。如果有答案，请回答不超过20个字的文本。\n",
    "问题：智慧知冷暖是如何运行的的最简短答案是：\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用使用PLD的生成函数：（每一行表示一次模型推理结果）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3332676/1086294788.py:185: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  model_inputs[\"position_ids\"] = torch.unsqueeze(torch.range(0, candidate_input_ids.shape[1]-1, 1, dtype=torch.int64), dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop <system>你是一个AI助手，旨在提供帮助、诚实和无害的对话服务。<user>风吹人，你能解释一下吗<assistant>风吹人是通过空调摄像头，计算出人体实时位置，仅对该区域进行送风。送风区域可以跟随人体走动而随时变化，人动，风也动。</s><user>风吹人功能在空调上要怎么进行操作<assistant>空调的风吹人功能是通过空调摄像头计算出人体的实时位置，然后仅对该区域进行送风。当人体移动时，送风区域也会随之改变，实现人动风也动的效果。</s><user>风避人的原理是什么，怎么做到的<assistant>风避人通过空调摄像头计算出人体实时位置，让风避开人体位置。避风区域能随人体走动而变化，人在哪，避风区域就在哪。</s><user>可供参考的知识库有：\n",
      "问题:智慧知冷暖是什么的答案是智慧知冷暖是在制冷和制热模式下，红外传感器在有效检测范围内检测房间空气温度和人体表温度，空调结合房间空气湿度、空调送风状态等参数，计算人体冷热感。空调根据冷热感，自动调整运行参数，不用人手动遥控空调，让房间空气温度达到让人体感到舒适的状态。\n",
      "问题:智慧知冷暖的运行参数是如何自动调整的的答案是智慧知冷暖通过红外传感器检测房间空气温度和人体表温度，结合房间空气湿度、空调送风状态等参数，计算人体冷热感。然后根据这些数据，自动调整运行参数，使得房间空气温度达到让人体感到舒适的状态，无需人工手动调整。\n",
      "知识点:风避人是通过空调摄像头计算出人体实时位置，让风避开人体位置，避风区域可以根据人体的走动而变化。\n",
      "知识点:空调的风避人功能是通过摄像头计算出人体实时位置，让风避开人体。避风区域可以随着人体走动而变化，实现人在哪，避风区域就在哪的效果。\n",
      "知识点:风避人是通过空调摄像头，计算出人体实时位置，让风避开人体位置。避风区域可以跟随人体走动而随时变化，人在哪里，避风区域就在哪里。\n",
      "知识点:风避人功能通过空调摄像头计算出人体实时位置，避风区域会跟随人体走动而变化，即人在哪里，避风区域就在哪里。\n",
      "知识点:风避人通过空调摄像头计算出人体实时位置，然后让风避开人体位置。避风区域能随人体走动而变化，人在哪，避风区域就在哪。\n",
      "问题:智慧知冷暖的运行方式是怎样的的答案是智慧知冷暖通过红外传感器检测房间空气温度和人体表温度，结合房间空气湿度和空调送风状态等参数，计算人体冷热感。然后根据这个冷热感，自动调整空调运行参数，使得房间温度达到人体感到舒适的状态，无需人手动遥控。\n",
      "问题:智慧知冷暖是怎么运作的的答案是智慧知冷暖通过红外传感器检测房间空气温度和人体表温度，结合房间空气湿度和空调送风状态等参数，计算人体冷热感。然后根据这个冷热感，自动调整空调运行参数，使得房间温度达到人体感到舒适的状态，无需人手动遥控。\n",
      "根据这些搜索结果回答关于空调的问题。如果知识库中没有答案，请拒绝回答。如果有答案，请回答不超过20个字的文本。\n",
      "问题：智慧知冷暖是如何运行的的最简短答案是：智慧\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3332676/1086294788.py:191: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  model_inputs[\"position_ids\"] = torch.unsqueeze(torch.range(0, candidate_input_ids.shape[1]-1, 1, dtype=torch.int64), dim=0)[..., min(-candidate_length-1, -1):]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "知冷暖通过\n",
      "红外传感器检测房间空气温度和人体表温度，\n",
      "结合\n",
      "房间空气湿\n",
      "度和空调送风状态等参数，计算人体冷\n",
      "热感。然后\n",
      "根据这个\n",
      "冷热感，自动调整空调运行参数，使得\n",
      "房间温度\n",
      "达到人体感到舒适的状态，无需人手动遥控\n",
      "。\n",
      "\n",
      "共耗时=41.223499059677124 \n",
      "*************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for p in prompts:\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(device)\n",
    "    inputs['max_length'] = 2000\n",
    "    inputs['is_first_forward'] = True\n",
    "    stream_generate_pld = model.stream_generate_assisted_by_prompt_lookup_decoding(**inputs)\n",
    "    cur_len = 0\n",
    "    s_time = time.time()\n",
    "    for i in stream_generate_pld:\n",
    "        new_words = tokenizer.decode(i[0][cur_len:].tolist())\n",
    "        cur_len = len(i[0])\n",
    "        print(new_words)\n",
    "        # print(tokenizer.decode(i[0].tolist()))\n",
    "    print(f'共耗时={(time.time() - s_time)} ')\n",
    "    print(\"*************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用chatglm原流失生成函数 （每一行表示一次推理结果）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]sop <system>你是一个AI助手，旨在提供帮助、诚实和无害的对话服务。<user>风吹人，你能解释一下吗<assistant>风吹人是通过空调摄像头，计算出人体实时位置，仅对该区域进行送风。送风区域可以跟随人体走动而随时变化，人动，风也动。</s><user>风吹人功能在空调上要怎么进行操作<assistant>空调的风吹人功能是通过空调摄像头计算出人体的实时位置，然后仅对该区域进行送风。当人体移动时，送风区域也会随之改变，实现人动风也动的效果。</s><user>风避人的原理是什么，怎么做到的<assistant>风避人通过空调摄像头计算出人体实时位置，让风避开人体位置。避风区域能随人体走动而变化，人在哪，避风区域就在哪。</s><user>可供参考的知识库有：\n",
      "问题:智慧知冷暖是什么的答案是智慧知冷暖是在制冷和制热模式下，红外传感器在有效检测范围内检测房间空气温度和人体表温度，空调结合房间空气湿度、空调送风状态等参数，计算人体冷热感。空调根据冷热感，自动调整运行参数，不用人手动遥控空调，让房间空气温度达到让人体感到舒适的状态。\n",
      "问题:智慧知冷暖的运行参数是如何自动调整的的答案是智慧知冷暖通过红外传感器检测房间空气温度和人体表温度，结合房间空气湿度、空调送风状态等参数，计算人体冷热感。然后根据这些数据，自动调整运行参数，使得房间空气温度达到让人体感到舒适的状态，无需人工手动调整。\n",
      "知识点:风避人是通过空调摄像头计算出人体实时位置，让风避开人体位置，避风区域可以根据人体的走动而变化。\n",
      "知识点:空调的风避人功能是通过摄像头计算出人体实时位置，让风避开人体。避风区域可以随着人体走动而变化，实现人在哪，避风区域就在哪的效果。\n",
      "知识点:风避人是通过空调摄像头，计算出人体实时位置，让风避开人体位置。避风区域可以跟随人体走动而随时变化，人在哪里，避风区域就在哪里。\n",
      "知识点:风避人功能通过空调摄像头计算出人体实时位置，避风区域会跟随人体走动而变化，即人在哪里，避风区域就在哪里。\n",
      "知识点:风避人通过空调摄像头计算出人体实时位置，然后让风避开人体位置。避风区域能随人体走动而变化，人在哪，避风区域就在哪。\n",
      "问题:智慧知冷暖的运行方式是怎样的的答案是智慧知冷暖通过红外传感器检测房间空气温度和人体表温度，结合房间空气湿度和空调送风状态等参数，计算人体冷热感。然后根据这个冷热感，自动调整空调运行参数，使得房间温度达到人体感到舒适的状态，无需人手动遥控。\n",
      "问题:智慧知冷暖是怎么运作的的答案是智慧知冷暖通过红外传感器检测房间空气温度和人体表温度，结合房间空气湿度和空调送风状态等参数，计算人体冷热感。然后根据这个冷热感，自动调整空调运行参数，使得房间温度达到人体感到舒适的状态，无需人手动遥控。\n",
      "根据这些搜索结果回答关于空调的问题。如果知识库中没有答案，请拒绝回答。如果有答案，请回答不超过20个字的文本。\n",
      "问题：智慧知冷暖是如何运行的的最简短答案是：智慧\n",
      "知\n",
      "冷暖\n",
      "通过\n",
      "红外\n",
      "传感器\n",
      "检测\n",
      "房间\n",
      "空气\n",
      "温度\n",
      "和\n",
      "人体\n",
      "表\n",
      "温度\n",
      "，\n",
      "结合\n",
      "房间\n",
      "空气\n",
      "湿\n",
      "度和\n",
      "空调\n",
      "送\n",
      "风\n",
      "状态\n",
      "等\n",
      "参数\n",
      "，\n",
      "计算\n",
      "人体\n",
      "冷\n",
      "热\n",
      "感\n",
      "。\n",
      "然后\n",
      "根据\n",
      "这个\n",
      "冷\n",
      "热\n",
      "感\n",
      "，\n",
      "自动\n",
      "调整\n",
      "空调\n",
      "运行\n",
      "参数\n",
      "，\n",
      "使得\n",
      "房间\n",
      "温度\n",
      "达到\n",
      "人体\n",
      "感到\n",
      "舒适\n",
      "的状态\n",
      "，\n",
      "无需\n",
      "人\n",
      "手动\n",
      "遥\n",
      "控\n",
      "。\n",
      "\n",
      "共耗时=56.89959716796875 \n",
      "*************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for p in prompts:\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(device)\n",
    "    inputs['max_length'] = 2000\n",
    "    stream_generate = model.stream_generate(**inputs)\n",
    "    cur_len = 0\n",
    "    s_time = time.time()\n",
    "    for i in stream_generate:\n",
    "        new_words = tokenizer.decode(i[0][cur_len:].tolist())\n",
    "        cur_len = len(i[0])\n",
    "        print(new_words)\n",
    "        # print(tokenizer.decode(i[0].tolist()))\n",
    "    print(f'共耗时={(time.time() - s_time)} ')\n",
    "    print(\"*************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单次测试脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "inputs['max_length'] = 2000\n",
    "inputs['is_first_forward'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_generate = model.stream_generate(**inputs)\n",
    "cur_len = 0\n",
    "s_time = time.time()\n",
    "for i in stream_generate:\n",
    "    new_words = tokenizer.decode(i[0][cur_len:].tolist())\n",
    "    cur_len = len(i[0])\n",
    "    print(new_words)\n",
    "    print(f'单次流式耗时={(time.time() - s_time)} ')\n",
    "# print(tokenizer.decode(i[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_generate_pld = model.stream_generate_assisted_by_prompt_lookup_decoding(**inputs)\n",
    "\n",
    "cur_len = 0\n",
    "s_time = time.time()\n",
    "for i in stream_generate_pld:\n",
    "    new_words = tokenizer.decode(i[0][cur_len:].tolist())\n",
    "    cur_len = len(i[0])\n",
    "    print(new_words)\n",
    "    print(f'单次流式耗时={(time.time() - s_time)} ')\n",
    "    # print(tokenizer.decode(i[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weiqing-cookgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
